# üõ°Ô∏è D√âFENSE PARTIE 3 : ARCHITECTURE DU MOD√àLE

**Membre Responsable :** [Nom du Membre 3]
**Objectif :** Pr√©senter le "Cerveau" artificiel.

---

## 1. Vue d'ensemble : Le Perceptron Multicouche (MLP)
Nous avons choisi une architecture simple mais robuste pour ces donn√©es tabulaires.

```mermaid
graph LR
    Input[Entr√©e (20 Features)] --> Hidden1[Cach√©e 1 (64 Neurones)]
    Hidden1 --> Hidden2[Cach√©e 2 (32 Neurones)]
    Hidden2 --> Hidden3[Cach√©e 3 (16 Neurones)]
    Hidden3 --> Output[Sortie (1 Logit)]
```

## 2. Les Composants (`src/model/nf1_classifier.py`)

### A. Linear (Fully Connected)
Ce sont les neurones qui font les calculs ($y = wx + b$).
*   Couche 1 : 20 ant√©c√©dents -> 64 neurones.
*   Couche 2 : 64 -> 32.
*   Couche 3 : 32 -> 16.

### B. Activation (ReLU)
Sans activation, le r√©seau ne serait qu'une r√©gression lin√©aire g√©ante. **ReLU** (Rectified Linear Unit) permet d'apprendre des relations non-lin√©aires complexes.

### C. Stabilisation & G√©n√©ralisation
*   **Batch Normalization** : Recentre les donn√©es entre chaque couche.
    *   *Pourquoi ?* √áa acc√©l√®re l'apprentissage et √©vite que les neurones ne "meurent".
*   **Dropout (0.3)** : On √©teint al√©atoirement 30% des neurones √† chaque passage.
    *   *Pourquoi ?* Pour forcer le r√©seau √† √™tre robuste et ne pas apprendre par c≈ìur (Overfitting).

## 3. Le Point Critique : "Logits" vs "Probabilit√©s"
Initialement, nous avions une `Sigmoid` √† la fin pour sortir une probabilit√© (0 √† 1).
**Nous l'avons retir√©e.**

> *"Pourquoi ce changement ?"*
> Pour utiliser la fonction de perte `BCEWithLogitsLoss`. Elle combine la Sigmoid et la Loss en une seule op√©ration math√©matique plus stable num√©riquement. Le mod√®le sort donc un "Logit" (nombre brut, ex: 2.5 ou -1.2) que nous transformons en probabilit√© seulement au moment de la pr√©diction.

## üìã Conclusion pour cette partie
"Nous avons construit un r√©seau profond mais contr√¥l√© (Dropout/BatchNorm) et optimis√© pour la stabilit√© num√©rique. Il est pr√™t √† apprendre."
